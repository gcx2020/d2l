{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c916bc51-693f-4aec-b0a9-7118c0cb5efd",
   "metadata": {},
   "source": [
    "## 2.3. 线性代数\n",
    "在介绍完如何存储和操作数据后，接下来将简要地回顾一下部分基本线性代数内容。 这些内容有助于读者了解和实现本书中介绍的大多数模型。 本节将介绍线性代数中的基本数学对象、算术和运算，并用数学符号和相应的代码实现来表示它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd7644-f43f-4bed-8789-9b76b66a053a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3.1. 标量\n",
    "如果你曾经在餐厅支付餐费，那么应该已经知道一些基本的线性代数，比如在数字间相加或相乘。 例如，北京的温度为 $52^oF$\n",
    "（华氏度，除摄氏度外的另一种温度计量单位）。 严格来说，仅包含一个数值被称为标量（scalar）。 如果要将此华氏度值转换为更常用的摄氏度， 则可以计算表达式 $c = \\frac{5}{9}(f-32)$，并将f赋为52在此等式中，每一项（5、9和32）都是标量值。 符号c和f称为变量（variable），它们表示未知的标量值。\n",
    "本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，x、y和z）。 本书用R表示所有（连续）实数标量的空间，之后将严格定义空间（space）是什么， 但现在只要记住表达式$x \\in R$是表示是一个实值标量的正式形式。$\\in$ 符号称为“属于”，它表示“是集合中的成员”。 例如$x,y \\in  \\{0,1\\}$可以用来表明x和y是值只能为0或1的数字。\n",
    "标量由只有一个元素的张量表示。 下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e01796f-3c08-4822-9d71-fbefb39e76d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11827ffc-0638-4d99-8843-43e54d49dc3d",
   "metadata": {},
   "source": [
    "### 2.3.2. 向量\n",
    "向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。 当向量表示数据集中的样本时，它们的值具有一定的现实意义。 例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联， 其分量与其收入、工作年限、过往违约次数和其他因素相对应。 如果我们正在研究医院患者可能面临的心脏病发作风险，可能会用一个向量来表示每个患者， 其分量为最近的生命体征、胆固醇水平、每天运动时间等。 在数学表示法中，向量通常记为粗体、小写的符号 （例如，x、y和z）。\n",
    "人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfebeda-5487-4ec6-a2b8-83fe4c1cd7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(40)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028c698-70f0-423c-be3c-20caee491e21",
   "metadata": {},
   "source": [
    "我们可以使用下标来引用向量的任一元素，例如可以通过\n",
    "来引用第\n",
    "个元素。 注意，元素\n",
    "是一个标量，所以我们在引用它时不会加粗。 大量文献认为列向量是向量的默认方向，在本书中也是如此。 在数学中，向量\n",
    "可以写为：\n",
    "\\begin{split}\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},\\end{split}\n",
    "其中 $x_1,\\ldots,x_n$ 是向量的元素。在代码中，我们通过张量的索引访问任一元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35638d69-d564-452b-9bc7-0202c07a391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91532b4-e38b-4af1-ada3-3c341d43b8c2",
   "metadata": {},
   "source": [
    "#### 2.3.2.1. 长度、维度和形状\n",
    "向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 在数学表示法中，如果我们想说一个向量$\\mathbf{x}$由$n$个实值标量组成， 可以将其表示为\n",
    "$\\mathbf{x}\\in\\mathbb{R}^n$。 向量的长度通常称为向量的维度（dimension）。\n",
    "与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f1627c-e83a-459c-82b9-92eb640d4f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cc04d-080a-4135-8fa3-723b0ff66670",
   "metadata": {},
   "source": [
    "当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3c9a46-b087-403f-808c-3c4871f75ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498aba39-d517-4a4f-938a-931813f3a123",
   "metadata": {},
   "source": [
    "请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： **向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量**。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f13edd-71da-4438-af02-687bc35bc1cd",
   "metadata": {},
   "source": [
    "### 2.3.3. 矩阵\n",
    "正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。 矩阵，我们通常用粗体、大写字母来表示 （例如，$\\mathbf{X}$,$\\mathbf{Y}$和$\\mathbf{Z}$）在代码中表示为具有两个轴的张量。\n",
    "数学表示法使用$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$来表示矩阵$\\mathbf{A}$,其由$m$行和$n$列的实值标量组成。 我们可以将任意矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$视为一个表格， 其中每个元素$a_{ij}$属于第$i$行第$j$列：\n",
    "\\begin{split}\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.\\end{split}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255c9649-5907-47f7-99d0-c9af9f7a9622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df49c076-470c-4368-a533-fcdf8cfc5d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5,-1)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec20aa-21d2-4c1a-be8c-30bdb9c6c84f",
   "metadata": {},
   "source": [
    "我们可以通过行索引（$i$）和列索引（$j$）来访问矩阵中的标量元素$a_{ij}$， 例如$[\\mathbf{A}]_{ij}$\n",
    "。 如果没有给出矩阵$\\mathbf{A}$\n",
    "的标量元素，如在 (2.3.2)那样， 我们可以简单地使用矩阵$\\mathbf{A}$\n",
    "的小写字母索引下标$a_{ij}$来引用$[\\mathbf{A}]_{ij}$。为了表示起来简单，只有在必要时才会将逗号插入到单独的索引中， 例如$a_{2,3j}$和$[\\mathbf{A}]_{2i-1,3}$。\n",
    "当我们交换矩阵的行和列时，结果称为矩阵的转置（transpose）。 通常用$\\mathbf{a}^\\top$来表示矩阵的转置，如果$\\mathbf{B}=\\mathbf{A}^\\top$ 则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$。 因此，在(2.3.2)中的转置是一个形状为的矩阵：\n",
    "\\begin{split}\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\\end{split}\n",
    "现在在代码中访问矩阵的转置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6820f84-a185-4d43-b630-79cf9aa32155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72e981-5417-4ea4-abf8-73e7bb655e3e",
   "metadata": {},
   "source": [
    "作为方阵的一种特殊类型，对称矩阵（symmetric matrix）,$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$。 这里定义一个对称矩阵$\\mathbf{B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a517941c-593a-4d53-9828-e06acc2a2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3a8888-7245-47b6-b7e6-c5ef8b24f0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba702768-c79c-4c88-a0a7-b7563ba3f48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ad6aa-63ce-4858-a926-585a5762d260",
   "metadata": {},
   "source": [
    "矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。 例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。 曾经使用过电子表格软件或已阅读过 2.2节的人，应该对此很熟悉。 因此，尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。 后面的章节将讲到这点，这种约定将支持常见的深度学习实践。 例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ef199-52ac-4e55-8048-9f94541e4bc3",
   "metadata": {},
   "source": [
    "### 2.3.4. 张量\n",
    "就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的\n",
    "维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。 张量用特殊字体的大写字母表示（例如，$\\mathsf{X}$、$\\mathsf{Y}$和$\\mathsf{Z}$）， 它们的索引机制（例如$x_{ijk}$和$[\\mathsf{X}]_{1,2i-1,3}$）与矩阵类似。\n",
    "当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。 现在先将高阶张量暂放一边，而是专注学习其基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf62573c-ea91-4e97-a0d1-a7f5a9cfdc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d0fb0-5518-40c1-a1b6-c5c52be24036",
   "metadata": {},
   "source": [
    "### 2.3.5. 张量算法的基本性质\n",
    "标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bba2d48-ec87-451e-aa70-f2e82678cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ef3dd8-c2b6-48e2-9b24-106632370a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,-1)\n",
    "B = A.clone() # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d6592-7e1c-4498-8a9c-298a33b420ea",
   "metadata": {},
   "source": [
    "具体而言，两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号$\\odot$)。对于矩阵$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，， 其中第$i$行和第$j$列的元素是$b_{ij}$。矩阵\n",
    "$\\mathbf{A}$（在 (2.3.2)中定义）和 $\\mathbf{B}$的Hadamard积为：\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b61051-cd91-42b8-a144-a3f0a7f0ee7b",
   "metadata": {},
   "source": [
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c289cb3-bb25-48e6-8567-8c2253c03c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "046d99c2-5db2-4c7b-b39a-ad0d98b17265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " tensor([[[ 0,  2,  4,  6],\n",
       "          [ 8, 10, 12, 14],\n",
       "          [16, 18, 20, 22]],\n",
       " \n",
       "         [[24, 26, 28, 30],\n",
       "          [32, 34, 36, 38],\n",
       "          [40, 42, 44, 46]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X, a + X, a * X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bccc0b-d586-4974-9cf8-91e3f03c9142",
   "metadata": {},
   "source": [
    "### 2.3.6. 降维\n",
    "我们可以对任意张量进行的一个有用的操作是计算其元素的和。 数学表示法使用$\\sum$符号表示求和。 为了表示长度为$d$的向量中元素的总和，可以记为$\\sum_{i=1}^dx_i$。 在代码中可以调用计算求和的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c4c31d-44ce-44bf-8556-e87b852fc1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.), 6.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x, x.sum() , x.sum().item() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46968ab4-aa38-46b2-a8bb-23c64e947ebe",
   "metadata": {},
   "source": [
    "我们可以表示任意形状张量的元素和。 例如，矩阵$\\mathbf{A}$中元素标记为$\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f6df14-2bf7-4e66-ab86-59a8374ed398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), tensor(276), 276)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(24).reshape(2,3,-1)\n",
    "A.shape , A.sum(), A.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47811b7-198a-4f8d-b6d2-e2e1306d035e",
   "metadata": {},
   "source": [
    "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97203a13-d7f6-43c6-82dc-368898329a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis = 0)\n",
    "A, A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7b730-1391-460a-86fa-a155bbe4caf2",
   "metadata": {},
   "source": [
    "指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4592a97b-f59c-42ac-9ab4-8de804204d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis = 1)\n",
    "A, A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4530bb-1f8b-4c3b-920a-7128364e9dab",
   "metadata": {},
   "source": [
    "沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43195a48-5d85-4e54-a98f-02306bba9acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(276)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis = [0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d1214-5c17-41ca-a6c8-775139236255",
   "metadata": {},
   "source": [
    "一个与求和相关的量是平均值（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634ff19a-1d53-449c-b425-b77cdf101b57",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.5000), tensor(11.5000))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(24).reshape(2,3,-1)\n",
    "A = A.to(torch.float32)\n",
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbb260-d19a-42d4-8f5a-ffa826bf53cc",
   "metadata": {},
   "source": [
    "同样，计算平均值的函数也可以沿指定轴降低张量的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf483733-78bf-42b8-bd0d-20adff5eaaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.]]]),\n",
       " tensor([[ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.]]),\n",
       " tensor([[ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74432e39-a245-4e5a-8d8c-50f628717b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "tensor([[10657.4189,  8845.9219, 19283.6816,  ..., 12513.9531,  3037.3147,\n",
      "          6307.4766],\n",
      "        [10947.6406, 12767.6289,  5762.2847,  ...,  6624.8115,  8579.5107,\n",
      "          6212.9224],\n",
      "        [10000.7588, 13379.9258,  9783.2305,  ..., 11081.6904,  2783.9419,\n",
      "         10310.5039],\n",
      "        ...,\n",
      "        [ 8660.2861,  4021.6187, 15487.1230,  ...,  6541.7690, 16086.5977,\n",
      "         11203.7705],\n",
      "        [ 6209.0430, 13536.4365, 12224.8301,  ...,  2737.7659, 14564.3291,\n",
      "          7733.9346],\n",
      "        [ 1976.6279, 11752.3203, 10034.3965,  ...,  7419.2861,  8959.4639,\n",
      "         16086.0615]], device='cuda:0')\n",
      "运行结束, 初始化使用了 3.88041353225708 秒, 循环用了 123.2205798625946 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "gpu = torch.device('cuda')\n",
    "# 如果用cpu测试那么注释掉上面的代码, 用下面的\n",
    "# gpu = torch.device('cpu')\n",
    "\n",
    "beginTime=time.time()\n",
    "\n",
    "a=torch.rand(20480,20480)\n",
    "b=torch.rand(20480,20480)\n",
    "c=torch.rand(20480,20480)\n",
    "\n",
    "x = a.to(gpu)\n",
    "y = b.to(gpu)\n",
    "\n",
    "z = c.to(gpu)\n",
    "\n",
    "initTime=time.time()\n",
    "print(\"ok\")\n",
    "\n",
    "i=0\n",
    "\n",
    "while i<10000:\n",
    "    z=(z+x+y)\n",
    "    i+=1\n",
    "    \n",
    "endTime=time.time()\n",
    "\n",
    "print(z)\n",
    "\n",
    "print(\"运行结束, 初始化使用了 {} 秒, 循环用了 {} 秒\".format(initTime-beginTime,endTime-beginTime))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc8ee7-af4e-4ab8-acc4-7733c2193f87",
   "metadata": {},
   "source": [
    "#### 2.3.6.1. 非降维求和\n",
    "但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6856633-9080-48a9-8531-8e4564111638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.]]]),\n",
       " tensor([[ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.]]),\n",
       " tensor([[[12., 14., 16., 18.],\n",
       "          [20., 22., 24., 26.],\n",
       "          [28., 30., 32., 34.]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(24).reshape(2,3,-1)\n",
    "A = A.to(torch.float32)\n",
    "A,A.mean(axis=0), A.sum(axis=0,keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd3629-c784-4b1f-8ef5-6f6a759bde5f",
   "metadata": {},
   "source": [
    "例如，由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a471b5-50e7-4eed-9030-b7fd95e7cb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0714, 0.1250, 0.1667],\n",
       "         [0.2000, 0.2273, 0.2500, 0.2692],\n",
       "         [0.2857, 0.3000, 0.3125, 0.3235]],\n",
       "\n",
       "        [[1.0000, 0.9286, 0.8750, 0.8333],\n",
       "         [0.8000, 0.7727, 0.7500, 0.7308],\n",
       "         [0.7143, 0.7000, 0.6875, 0.6765]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / A.sum(axis=0,keepdims = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75649d7e-f6f2-4ad3-8d14-83cfe45add96",
   "metadata": {},
   "source": [
    "如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cdfa431-027a-491e-afb6-bbaea496b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.]]]),\n",
       " tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 14., 16., 18.],\n",
       "          [20., 22., 24., 26.],\n",
       "          [28., 30., 32., 34.]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee76a3-39a4-454b-94a0-b3636b38097e",
   "metadata": {},
   "source": [
    "### 2.3.7. 点积（Dot Product）\n",
    "我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。 给定两个向量$\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$， 它们的点积（dot product）$\\mathbf{x}^\\top\\mathbf{y}$或$\\langle\\mathbf{x},\\mathbf{y}\\rangle$\n",
    "） 是相同位置的按元素乘积的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$\n",
    "。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5a305f7-ecd5-4af3-affe-15ec9dfd1dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = torch.ones(4,dtype = torch.float32)\n",
    "x = torch.arange(4,dtype = torch.float32)\n",
    "x,y,torch.dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c70f9-c589-4f02-b99b-9cad25ceeb25",
   "metadata": {},
   "source": [
    "注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c589a00-05bd-4b3b-9b3b-d050523b1e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0deea4-e1f3-44ec-aefe-8dbf8f28a52c",
   "metadata": {},
   "source": [
    "点积在很多场合都很有用。 例如，给定一组由向量$\\mathbf{x} \\in \\mathbb{R}^d$表示的值， 和一组由$\\mathbf{w} \\in \\mathbb{R}^d$表示的权重。$\\mathbf{x}$ 中的值根据权重$\\mathbf{w}$的加权和，可以表示为点积$\\mathbf{x}^\\top \\mathbf{w}$。 当权重为非负数且和为1（即$\\left(\\sum_{i=1}^{d}{w_i}=1\\right)$）时， 点积表示加权平均（weighted average）。 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。 本节后面的内容将正式介绍长度（length）的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4f78b-71e5-42f0-b23e-45189612f5a4",
   "metadata": {},
   "source": [
    "### 2.3.8. 矩阵-向量积\n",
    "现在我们知道如何计算点积，可以开始理解矩阵-向量积（matrix-vector product）。 回顾分别在 (2.3.2)和 (2.3.1)中定义的矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$和向量$\\mathbf{x} \\in \\mathbb{R}^n$。 让我们将矩阵$\\mathbf{A}$用它的行向量表示：\n",
    "\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},\\end{split}\n",
    " \n",
    "其中每个$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$都是行向量，表示矩阵的第$i$行。 矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量， 其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$:\n",
    "\n",
    "\\begin{split}\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    " \n",
    "我们可以把一个矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$乘法看作一个从$\\mathbb{R}^{n}$到$\\mathbb{R}^{m}$的向量转换。这些转换是非常有用的，例如可以用方阵的乘法来表示旋转。 后续章节将讲到，我们也可以使用矩阵-向量积来描述在给定前一层的值时， 求解神经网络每一层所需的复杂计算。\n",
    "在代码中使用张量表示矩阵-向量积，我们使用mv函数。 当我们为矩阵A和向量x调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efea2425-e7f3-48c1-84bd-9dde341ea75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19]]),\n",
       " tensor([0, 1, 2, 3]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4]),\n",
       " tensor([ 14,  38,  62,  86, 110]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "x = torch.arange(4)\n",
    "A,x,A.shape,x.shape,torch.mv(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fedd5-6a38-49c7-b6af-f176474933e3",
   "metadata": {},
   "source": [
    "### 2.3.9. 矩阵-矩阵乘法\n",
    "在掌握点积和矩阵-向量积的知识后， 那么**矩阵-矩阵乘法**（matrix-matrix multiplication）应该很简单。\n",
    "假设有两个矩阵$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$和$\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$：\n",
    "\\begin{split}\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    "  \n",
    "用行向量$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$表示矩阵$\\mathbf{A}$的第$i$行，并让列向量$\\mathbf{b}_{j} \\in \\mathbb{R}^k$作为矩阵$\\mathbf{B}$的第$j$列。要生成矩阵积$\\mathbf{C} = \\mathbf{A}\\mathbf{B}$，最简单的方法是考虑$\\mathbf{A}$的行向量和$\\mathbf{B}$的列向量:\n",
    "\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    " \n",
    "当我们简单地将每个元素$c_{ij}$计算为点积$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\\end{split}\n",
    "我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \\times m$矩阵。 在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b811db-a422-4346-bd77-a16c30e70689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20,dtype = torch.float32).reshape(5,-1)\n",
    "B = torch.ones(4,3,dtype = torch.float32)\n",
    "\n",
    "A,B,A.shape,B.shape,torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ce236-297e-42a8-b585-e6b0ffab88a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123613da-4a92-4a22-9491-a39c11b6370c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba38db-f823-402e-a937-81a435f0d69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1609d3-e1bb-4326-aa11-9949981dc9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
