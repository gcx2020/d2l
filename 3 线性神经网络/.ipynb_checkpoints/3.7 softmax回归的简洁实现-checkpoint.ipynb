{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7dff1c-ea60-4ab1-aeb7-983e28771cd8",
   "metadata": {},
   "source": [
    "## 3.7. softmax回归的简洁实现\n",
    "在 3.3节中，我们发现通过深度学习框架的高级`API`能够使实现线性回归变得更加容易。同样，通过深度学习框架的高级`API`也能更方便地实现`softmax`回归模型。 本节如在 `3.6`节中一样， 继续使用`Fashion-MNIST`数据集，并保持批量大小为`256`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab74201b-642f-4480-9461-38e6355f2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd0e8d-dd09-46ca-ac30-69f06874c763",
   "metadata": {},
   "source": [
    "### 3.7.1. 初始化模型参数\n",
    "如我们在`3.4`节所述，`softmax`回归的输出层是一个全连接层。因此，为了实现我们的模型，我们只需在`Sequential`中添加一个带有`10`个输出的全连接层。 同样，在这里`Sequential`并不是必要的，但它是实现深度模型的基础。我们仍然以均值`0`和标准差`0.01`随机初始化权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6586f238-e832-49ee-974c-210441074dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch 不会隐式地调整输入的形状。因此\n",
    "# 我们在线性层前定义了展平层（flatten）来调整网络输入的形状\n",
    "\n",
    "net = nn.Sequential(nn.Flatten() ,nn.Linear(784,10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4e99b-3c61-4cf7-b569-cb2052bd94e5",
   "metadata": {},
   "source": [
    "### 3.7.2. 重新审视Softmax的实现\n",
    "在前面`3.6`节的例子中，我们计算了模型的输出，然后将此输出送入交叉熵损失。从数学上讲，这是一件完全合理的事情。然而，从计算角度来看，指数可能会造成数值稳定性问题。\n",
    "回想一下，`softmax`函数$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$ ，其中$\\hat y_j$是预测的概率分布。$o_j$ 是未规范化的预测$\\mathbf{o}$的第$j$个元素。 如果$o_k$中的一些数值非常大，那么$\\exp(o_k)$可能大于数据类型容许的最大数字，即上溢（overflow）。 这将使分母或分子变为`inf`（无穷大），最后得到的是`0`、`inf`或`nan`（不是数字）的$\\hat y_j$。在这些情况下，我们无法得到一个明确定义的交叉熵值。\n",
    "\n",
    "解决这个问题的一个技巧是： 在继续softmax计算之前，先从所有$o_k$中减去$\\max(o_k)$。这里可以看到每个$o_k$按常数进行的移动不会改变softmax的返回值：\n",
    "\n",
    "$$\\begin{split}\\begin{aligned}\n",
    "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
    "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
    "\\end{aligned}\\end{split} .(3.7.1) $$\n",
    "  \n",
    "在减法和规范化步骤之后，可能有些$o_j - \\max(o_k)$具有较大的负值。由于精度受限，$\\exp(o_j - \\max(o_k))$将有接近零的值，即下溢（underflow）。 这些值可能会四舍五入为零，使$\\hat y_j$为零， 并且使得$\\log(\\hat y_j)$的值为`-inf`。 反向传播几步后，我们可能会发现自己面对一屏幕可怕的nan结果。\n",
    "尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。通过将`softmax`和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。如下面的等式所示，我们避免计算$\\exp(o_j - \\max(o_k))$， 而可以直接使用$o_j - \\max(o_k)$，因为$\\log(\\exp(\\cdot))$被抵消了。\n",
    "\n",
    "$$\\begin{split}\\begin{aligned}\n",
    "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
    "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
    "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
    "\\end{aligned}\\end{split} .(3.7.2)$$\n",
    "我们也希望保留传统的`softmax`函数，以备我们需要评估通过模型输出的概率。但是，我们没有将`softmax`概率传递到损失函数中，而是在交叉熵损失函数中传递未规范化的预测，并同时计算`softmax`及其对数，这是一种类似“LogSumExp技巧”的聪明方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3e9208-e6d2-4b2a-b65d-c2cc85fee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda4aad-e951-42f6-acfb-8c3cf032164e",
   "metadata": {},
   "source": [
    "### 3.7.3. 优化算法\n",
    "在这里，我们使用学习率为`0.1`的小批量随机梯度下降作为优化算法。这与我们在线性回归例子中的相同，这说明了优化器的普适性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da928c85-6dd3-4bf9-bd66-8ea6f0a334a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3090a48-858a-4432-8117-4ebfecd08f73",
   "metadata": {},
   "source": [
    "### 3.7.4. 训练\n",
    "接下来我们调用 3.6节中定义的训练函数来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79d1f55-efd7-4523-8737-6226afed49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y): #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis = 1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8466e928-d006-493d-8bb4-2318174571a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net,data_iter):#@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval() #将模型改为评估模式\n",
    "    metric = Accumulator(2) #正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            metric.add(accuracy(net(X) ,y), y.numel())\n",
    "    return metric[0] / metric[1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc02598-a9f7-4804-abc6-d51df07fd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator: #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a , b in zip(self.data,args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea5ea02-d865-499c-b046-f863a4aebdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net,train_iter,loss,updater): #@save\n",
    "    \"\"\"训练模型一个迭代周期\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X,y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat,y)\n",
    "        if isinstance(updater,torch.optim.Optimizer):\n",
    "            # 使用Pytorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()),accuracy(y_hat,y),y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2] , metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64100cd4-265a-428a-83c0-779b8fa2e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8985645d-38f0-4a86-9576-dd4a72ac1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net,train_iter,test_iter,loss,num_epochs,updater): #@save\n",
    "    \"\"\"训练模型（定义见第三章）\"\"\"\n",
    "    animator = Animator(xlabel = 'epoch', xlim = [1,num_epochs],ylim = [0.3,0.9],\n",
    "                       legend=['train loss','train_acc','test_acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net,train_iter,loss,updater)\n",
    "        test_acc = evaluate_accuracy(net,test_iter)\n",
    "        animator.add(epoch+1,train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5,train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7,train_acc\n",
    "    assert test_acc <=1 and test_acc > 0.7,test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d31d8-c5f7-4359-b431-4bf96fa9d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:9 : 0.0017706208392977713\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d275abb-0e66-4724-916c-f750a230b844",
   "metadata": {},
   "source": [
    "和以前一样，这个算法使结果收敛到一个相当高的精度，而且这次的代码比之前更精简了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9fc970-5df0-4a1e-bdb1-825169d8db90",
   "metadata": {},
   "source": [
    "### 3.7.5. 小结\n",
    "- 使用深度学习框架的高级`API`，我们可以更简洁地实现`softmax`回归。\n",
    "- 从计算的角度来看，实现`softmax`回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d415-be55-484e-8711-43ab99e97bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
